{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(path, threshold=3): \n",
    "    with open(path,'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        word_count = defaultdict(int)\n",
    "        tag_count = defaultdict(int)\n",
    "        word_count['<pad>']=0\n",
    "        word_count['<unk>']=0\n",
    "\n",
    "        sentences = [] #list of sentences\n",
    "        tagged_sentences = [] #tag of words sentences\n",
    "        sentence = [] #list of words in sentence\n",
    "        tagged_sentence = [] #list of tag for words in sentence\n",
    "\n",
    "        orig_sentences = [] #to retain original sentences\n",
    "        orig_sentence = [] #to retain original words\n",
    "\n",
    "        for line in lines:\n",
    "            data = line.split()          \n",
    "            if data: #row is not empty\n",
    "                word_count[data[1].lower()]+=1      #edit to test boolean mask\n",
    "                tag_count[data[2]]+=1\n",
    "        \n",
    "        for line in lines:\n",
    "            data = line.split()\n",
    "            if not data: #row is empty\n",
    "                sentences.append(sentence)\n",
    "                orig_sentences.append(orig_sentence)\n",
    "                tagged_sentences.append(tagged_sentence)\n",
    "                sentence = []\n",
    "                orig_sentence = []\n",
    "                tagged_sentence = []\n",
    "            else: \n",
    "                if word_count[data[1].lower()]<threshold:           #edit to test boolean mask\n",
    "                    word_count['<unk>'] += word_count[data[1]]\n",
    "                    del word_count[data[1]]\n",
    "                sentence.append(data[1].lower())\n",
    "                orig_sentence.append(data[1])\n",
    "                tagged_sentence.append(data[2])\n",
    "        sentences.append(sentence)\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "        orig_sentences.append(orig_sentence)\n",
    "    return orig_sentences, sentences, tagged_sentences, word_count, tag_count\n",
    "\n",
    "train_sentences, train_model_sentences, tagged_sentences, vocab, tag_count = create_vocab('data/train')\n",
    "\n",
    "word_list = list(vocab.keys()) \n",
    "tag_list = list(tag_count.keys())\n",
    "vocab_size = len(word_list)\n",
    "char_list = ['<pad>']\n",
    "char_list.extend([*\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_input(sentences, vocab, word_list):\n",
    "    embed_inp = [[word_list.index(word) if word in vocab else word_list.index('<unk>') for word in sentence] for sentence in sentences]\n",
    "    return embed_inp\n",
    "    \n",
    "def boolean_mask(sentences): #0 -> capital, 1 -> lower\n",
    "    return [[1 if word==word.lower() else 0 for word in sentence] for sentence in sentences]\n",
    "\n",
    "def encode_tags(tags):\n",
    "    return [[tag_list.index(tag) for tag in sentence] for sentence in tags]  \n",
    "\n",
    "def encode_char_sequence(sentences):\n",
    "    maxsentlen = -np.Inf\n",
    "    maxwordlen = -np.Inf\n",
    "    for sentence in sentences:\n",
    "        maxsentlen = max(maxsentlen, len(sentence))\n",
    "        for word in sentence:\n",
    "            maxwordlen = max(maxwordlen, len(word))\n",
    "    encoded_chars = torch.zeros(len(sentences),maxsentlen, maxwordlen)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, word in enumerate(sentence):\n",
    "            for k, char in enumerate(word):\n",
    "                encoded_chars[i][j][k] = char_list.index(char)\n",
    "    return encoded_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dev(path): #returns original sentences, sentences to be modelled, tags of sentences\n",
    "    with open(path,'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        model_sentences = [] #list of sentences\n",
    "        tagged_sentences = [] #tag of words sentences\n",
    "\n",
    "        model_sentence = [] #list of words in sentence\n",
    "        tagged_sentence = [] #list of tag for words in sentence\n",
    "\n",
    "        orig_sentences = [] #to retain original sentences\n",
    "        orig_sentence = []\n",
    "\n",
    "        for line in lines:\n",
    "            data = line.split()\n",
    "            if data: #row not empty\n",
    "                model_sentence.append(data[1].lower())\n",
    "                tagged_sentence.append(data[2])\n",
    "                orig_sentence.append(data[1])\n",
    "            else:\n",
    "                model_sentences.append(model_sentence)\n",
    "                tagged_sentences.append(tagged_sentence)\n",
    "                orig_sentences.append(orig_sentence)\n",
    "                model_sentence=[]\n",
    "                tagged_sentence=[]\n",
    "                orig_sentence=[]\n",
    "        model_sentences.append(model_sentence)\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "        orig_sentences.append(orig_sentence)\n",
    "    return orig_sentences, model_sentences, tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test(path):\n",
    "    with open(path,'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        model_sentences = [] #list of sentences\n",
    "        model_sentence = [] #list of words in sentence\n",
    "\n",
    "        orig_sentences = [] #to retain original sentences\n",
    "        orig_sentence = []\n",
    "\n",
    "        for line in lines:\n",
    "            data = line.split()\n",
    "            if data: #row not empty\n",
    "                model_sentence.append(data[1].lower())\n",
    "                orig_sentence.append(data[1])\n",
    "            else:\n",
    "                model_sentences.append(model_sentence)\n",
    "                orig_sentences.append(orig_sentence)\n",
    "                model_sentence=[]\n",
    "                orig_sentence=[]\n",
    "        model_sentences.append(model_sentence)\n",
    "        orig_sentences.append(orig_sentence)\n",
    "    return orig_sentences, model_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_weight = [1 if tag!='O' else 0.7 for tag in tag_list]\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(target_weight, device=device),ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, masks):\n",
    "        self.sentences = pad_sequence([torch.tensor(sentence) for sentence in sentences], batch_first=True, padding_value=0)\n",
    "        self.lengths = torch.tensor([len(sentence) for sentence in sentences])\n",
    "        self.tags = pad_sequence([torch.tensor(tag) for tag in tags], batch_first=True, padding_value=-1)\n",
    "        self.masks = pad_sequence([torch.tensor(mask) for mask in masks], batch_first=True, padding_value=2)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tag = self.tags[idx]\n",
    "        length = self.lengths[idx]\n",
    "        mask = self.masks[idx]\n",
    "        return sentence, length, tag, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTestDataset(Dataset):\n",
    "    def __init__(self, sentences, masks):\n",
    "        self.sentences = pad_sequence([torch.tensor(sentence) for sentence in sentences], batch_first=True, padding_value=0)\n",
    "        self.lengths = torch.tensor([len(sentence) for sentence in sentences])\n",
    "        self.masks = pad_sequence([torch.tensor(mask) for mask in masks], batch_first=True, padding_value=2)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        length = self.lengths[idx]\n",
    "        mask = self.masks[idx]\n",
    "        return sentence, length, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_LSTM(model, dataloader):\n",
    "    predicted_labels = []\n",
    "    valid_loss=0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, lens, labels, masks in dataloader:\n",
    "            packed_labels = pack_padded_sequence(labels, lens, batch_first=True, enforce_sorted=False)\n",
    "            labels, len_labels = pad_packed_sequence(packed_labels, batch_first=True, padding_value=-1)\n",
    "            batch, labels, masks = batch.to(device), labels.to(device), masks.to(device)\n",
    "            y_pred = model(batch, lens, masks)\n",
    "            y_pred = torch.permute(y_pred, dims=(0,2,1))\n",
    "            y_pred_class = torch.argmax(torch.log_softmax(y_pred, dim=1), dim=1)\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            valid_loss+= loss.item()\n",
    "            predicted_labels.extend(y_pred_class.cpu().numpy().tolist())\n",
    "        \n",
    "        valid_loss/=len(dataloader)\n",
    "    print('Validation Loss : ', valid_loss)\n",
    "    return predicted_labels, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLSTM(model_name, model, optimizer, train_dataloader, valid_dataloader, scheduler=None, epochs=10):\n",
    "    valid_loss_min = np.Inf\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch, lens, labels, masks in train_dataloader:\n",
    "            packed_labels = pack_padded_sequence(labels, lens, batch_first=True, enforce_sorted=False)\n",
    "            labels,_ = pad_packed_sequence(packed_labels, batch_first=True, padding_value=-1)\n",
    "            batch, labels, masks = batch.to(device), labels.to(device), masks.to(device)\n",
    "            y_pred = model(batch, lens, masks)\n",
    "            y_pred = torch.permute(y_pred, (0,2,1))\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss/=len(train_dataloader)\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(train_loss)\n",
    "        elif scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.4f} \\t'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        ))\n",
    "        _, valid_loss = evaluate_LSTM(model, valid_dataloader)\n",
    "        if valid_loss_min>valid_loss:\n",
    "            print(f'Validation Loss Reduced ({valid_loss_min})--->({valid_loss}). Saving model')\n",
    "            torch.save(model.state_dict(), f'{model_name}.pt')\n",
    "            valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_preds(model, dataloader):\n",
    "    predicted_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, lens, masks in dataloader:\n",
    "            batch, masks = batch.to(device), masks.to(device)\n",
    "            y_pred = model(batch, lens, masks)\n",
    "            y_pred = torch.permute(y_pred, dims=(0,2,1))\n",
    "            y_pred_class = torch.argmax(torch.log_softmax(y_pred, dim=1), dim=1)\n",
    "            predicted_labels.extend(y_pred_class.cpu().numpy().tolist())\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_train = embed_input(train_model_sentences, vocab, word_list)\n",
    "encoded_train = encode_tags(tagged_sentences)  \n",
    "train_masks = boolean_mask(train_sentences)\n",
    "\n",
    "train_data = LSTMDataset(embedded_train, encoded_train, train_masks)\n",
    "train_loader = DataLoader(train_data, batch_size=9, shuffle=True)\n",
    "\n",
    "orig_dev_sentences, dev_model_sentences, dev_tags = process_dev('data/dev')\n",
    "\n",
    "embedded_dev = embed_input(dev_model_sentences, vocab, word_list)\n",
    "encoded_dev = encode_tags(dev_tags)\n",
    "dev_masks = boolean_mask(orig_dev_sentences)\n",
    "\n",
    "valid_data = LSTMDataset(embedded_dev, encoded_dev, dev_masks)\n",
    "valid_loader = DataLoader(valid_data, batch_size=128)\n",
    "\n",
    "orig_test_sentences, test_model_sentences = process_test('data/test')\n",
    "\n",
    "embedded_test = embed_input(test_model_sentences, vocab, word_list)\n",
    "test_masks = boolean_mask(orig_test_sentences)\n",
    "\n",
    "test_data = LSTMTestDataset(embedded_test, test_masks)\n",
    "test_loader = DataLoader(test_data, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.3622 \t\n",
      "Validation Loss :  0.22366638348570891\n",
      "Validation Loss Reduced (inf)--->(0.22366638348570891). Saving model\n",
      "Epoch: 2 \tTraining Loss: 0.1892 \t\n",
      "Validation Loss :  0.16978095219071423\n",
      "Validation Loss Reduced (0.22366638348570891)--->(0.16978095219071423). Saving model\n",
      "Epoch: 3 \tTraining Loss: 0.1318 \t\n",
      "Validation Loss :  0.15521524513938598\n",
      "Validation Loss Reduced (0.16978095219071423)--->(0.15521524513938598). Saving model\n",
      "Epoch: 4 \tTraining Loss: 0.0945 \t\n",
      "Validation Loss :  0.1422420060262084\n",
      "Validation Loss Reduced (0.15521524513938598)--->(0.1422420060262084). Saving model\n",
      "Epoch: 5 \tTraining Loss: 0.0693 \t\n",
      "Validation Loss :  0.14214840145515545\n",
      "Validation Loss Reduced (0.1422420060262084)--->(0.14214840145515545). Saving model\n",
      "Epoch: 6 \tTraining Loss: 0.0526 \t\n",
      "Validation Loss :  0.15663559189332382\n",
      "Epoch: 7 \tTraining Loss: 0.0409 \t\n",
      "Validation Loss :  0.15785922283040627\n",
      "Epoch: 8 \tTraining Loss: 0.0323 \t\n",
      "Validation Loss :  0.16699592542967626\n",
      "Epoch: 9 \tTraining Loss: 0.0263 \t\n",
      "Validation Loss :  0.1774588830636016\n",
      "Epoch: 10 \tTraining Loss: 0.0211 \t\n",
      "Validation Loss :  0.17801679537764617\n"
     ]
    }
   ],
   "source": [
    "class BLSTM1(nn.Module):\n",
    "    def __init__(self, vocab_size, cap_embed_size=10):\n",
    "        super(BLSTM1, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, 100, padding_idx=0)\n",
    "        self.cap_embedding = nn.Embedding(3, cap_embed_size, padding_idx=0)\n",
    "        nn.init.uniform_(self.cap_embedding.weight, -np.sqrt(3/cap_embed_size), np.sqrt(3/cap_embed_size))\n",
    "        self.lstm = nn.LSTM(input_size=100+cap_embed_size, hidden_size=256, batch_first=True, bidirectional=True)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.uniform_(param, -np.sqrt(6/(len(param)+len(param[0]))), np.sqrt(6/(len(param)+len(param[0]))))\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear = nn.Linear(512, 128)\n",
    "        self.elu = nn.ELU()\n",
    "        self.out = nn.Linear(128,9)\n",
    "\n",
    "    def forward(self, x, lengths, masks):\n",
    "        # print('x shape ', x.shape)\n",
    "        output = self.embedding(x)\n",
    "        # print('after embedding ', output.shape)\n",
    "        cap_embeds = self.cap_embedding(masks)\n",
    "        output = torch.cat((output, cap_embeds), 2)\n",
    "        output = pack_padded_sequence(output, lengths, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.lstm(output)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True,padding_value=0)\n",
    "        # print('after lstm ', output.shape)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        # print('after linear ', output.shape)\n",
    "        output = self.elu(output)\n",
    "        output = self.out(output)\n",
    "        # print('after classifier ', output.shape)\n",
    "        return output\n",
    "\n",
    "blstm1 = BLSTM1(vocab_size).to(device)\n",
    "optimizer = torch.optim.SGD(blstm1.parameters(), lr=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',0.5)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, 0.01, 0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer,10,0.5)\n",
    "\n",
    "trainLSTM('blstm1', blstm1, optimizer, train_loader, valid_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss :  0.17801679657506092\n",
      "Validation Loss :  0.17583868971892766\n"
     ]
    }
   ],
   "source": [
    "blstmt = BLSTM1(vocab_size).to(device)\n",
    "blstmt.load_state_dict(torch.load('blstm1_final.pt'))\n",
    "predicted_labels, _ = evaluate_LSTM(blstm1, valid_loader)\n",
    "predicted_labels1, _ = evaluate_LSTM(blstmt, valid_loader)\n",
    "with open('predictions1.txt','w') as f:\n",
    "    for i, sentence in enumerate(dev_model_sentences):\n",
    "        for j, word in enumerate(sentence):\n",
    "            f.write(f'{j+1} {word} {dev_tags[i][j]} {tag_list[predicted_labels[i][j]]}\\n')\n",
    "        f.write('\\n')\n",
    "with open('predictions2.txt', 'w') as f:\n",
    "    for i, sentence in enumerate(dev_model_sentences):\n",
    "        for j, word in enumerate(sentence):\n",
    "            f.write(f'{j+1} {word} {dev_tags[i][j]} {tag_list[predicted_labels1[i][j]]}\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstmt = BLSTM1(vocab_size).to(device)\n",
    "blstmt.load_state_dict(torch.load('blstm1.pt'))\n",
    "\n",
    "valid_data = LSTMTestDataset(embedded_dev, dev_masks)\n",
    "valid_loader = DataLoader(valid_data, batch_size=128)\n",
    "\n",
    "predicted_labels = get_test_preds(blstmt, valid_loader)\n",
    "with open('dev1.out', 'w') as f:\n",
    "    for i, sentence in enumerate(orig_dev_sentences):\n",
    "        for j, word in enumerate(sentence):\n",
    "            f.write(f'{j+1} {word} {tag_list[predicted_labels[i][j]]}\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "orig_test_sentences, test_model_sentences = process_test('data/test')\n",
    "\n",
    "embedded_test = embed_input(test_model_sentences, vocab, word_list)\n",
    "test_masks = boolean_mask(orig_test_sentences)\n",
    "\n",
    "test_data = LSTMTestDataset(embedded_test, test_masks)\n",
    "test_loader = DataLoader(test_data, batch_size=128)\n",
    "\n",
    "predicted_labels = get_test_preds(blstmt, test_loader)\n",
    "with open('test1.out','w') as f:\n",
    "    for i, sentence in enumerate(orig_test_sentences):\n",
    "        for j, word in enumerate(sentence):\n",
    "            f.write(f'{j+1} {word} {tag_list[predicted_labels[i][j]]}\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_vocab(vocab):\n",
    "    with open('glove.6B.100d/glove.6B.100d.txt','r',encoding='utf-8') as f:\n",
    "        lines=f.readlines()\n",
    "        glove_vocab = defaultdict(list)\n",
    "        glove_vocab['<pad>']=np.zeros(100)\n",
    "        glove_vocab['<unk>']=np.random.uniform(-1,1,100)\n",
    "        for line in lines:\n",
    "            word, *embedding = line.split()\n",
    "            if word in vocab:\n",
    "                glove_vocab[word] = np.array(embedding).astype(np.double)\n",
    "        for w in list(set(vocab.keys())-set(glove_vocab.keys())):\n",
    "            glove_vocab[w] = np.random.uniform(-1,1,100)\n",
    "        return glove_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vocab = create_glove_vocab(vocab)\n",
    "glove_embeddings = list(glove_vocab.values())\n",
    "glove_list = list(glove_vocab.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_train = embed_input(train_model_sentences, glove_vocab, glove_list)\n",
    "train_masks = boolean_mask(train_model_sentences)\n",
    "encoded_train_tags = encode_tags(tagged_sentences)\n",
    "\n",
    "train_data = LSTMDataset(embedded_train, encoded_train_tags, train_masks)\n",
    "train_loader = DataLoader(train_data, batch_size=9, shuffle=True)\n",
    "\n",
    "orig_dev_sentences, dev_model_sentences, dev_tags = process_dev('data/dev')\n",
    "embedded_dev = embed_input(dev_model_sentences, glove_vocab, glove_list)\n",
    "dev_masks = boolean_mask(dev_model_sentences)\n",
    "encoded_dev = encode_tags(dev_tags)\n",
    "\n",
    "valid_data = LSTMDataset(embedded_dev, encoded_dev, dev_masks)\n",
    "valid_loader = DataLoader(valid_data, batch_size=128)\n",
    "\n",
    "orig_test_sentences, test_model_sentences = process_test('data/test')\n",
    "embedded_test = embed_input(test_model_sentences, glove_vocab, glove_list)\n",
    "test_masks = boolean_mask(orig_test_sentences)\n",
    "\n",
    "test_data = LSTMTestDataset(embedded_test, test_masks)\n",
    "test_loader = DataLoader(test_data, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM2(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings, cap_embed_size=10):\n",
    "        super(BLSTM2, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, 100, padding_idx=0)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embeddings, dtype=torch.float))\n",
    "        self.cap_embedding = nn.Embedding(3, cap_embed_size, padding_idx=2)\n",
    "        nn.init.uniform_(self.cap_embedding.weight, -np.sqrt(3/cap_embed_size), np.sqrt(3/cap_embed_size))\n",
    "        self.lstm = nn.LSTM(100+cap_embed_size, hidden_size=256, batch_first=True, bidirectional=True)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.uniform_(param, -np.sqrt(6/(len(param)+len(param[0]))), np.sqrt(6/(len(param)+len(param[0]))))\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear = nn.Linear(512, 128)\n",
    "        self.elu = nn.ELU()\n",
    "        self.out = nn.Linear(128,9)\n",
    "\n",
    "    def forward(self, x, lengths, masks):\n",
    "        # print('x shape ', x.shape)\n",
    "        output = self.embedding(x)\n",
    "        # print('after embedding ', output.shape)\n",
    "        cap_embeds = self.cap_embedding(masks)\n",
    "        # print('mask shape ', cap_embeds.shape)\n",
    "        output = torch.cat((output, cap_embeds), 2)\n",
    "        # print('concatenated shape ', output.shape)\n",
    "        output = pack_padded_sequence(output, lengths, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.lstm(output)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True,padding_value=0)\n",
    "        # print('after lstm ', output.shape)\n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        # print('after linear ', output.shape)\n",
    "        output = self.elu(output)\n",
    "        output = self.out(output)\n",
    "        # print('after classifier ', output.shape)\n",
    "        return output\n",
    "    \n",
    "blstm2 = BLSTM2(len(glove_list), glove_embeddings).to(device)\n",
    "optimizer2 = torch.optim.SGD(blstm2.parameters(), lr=0.5)\n",
    "scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer2,'min',0.5)\n",
    "# scheduler2 = torch.optim.lr_scheduler.CyclicLR(optimizer2, 0.01, 0.1)\n",
    "# scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer,5,0.5)\n",
    "        \n",
    "trainLSTM('blstm2', blstm2, optimizer2, train_loader, valid_loader, scheduler=scheduler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss :  2.0651948153972626\n"
     ]
    }
   ],
   "source": [
    "blstmt = BLSTM2(len(vocab), glove_embeddings).to(device)\n",
    "blstmt.load_state_dict(torch.load('blstm2.pt'))\n",
    "\n",
    "pred_labels1, _ = evaluate_LSTM(blstmt, valid_loader)\n",
    "with open('predictions3.txt','w') as f:\n",
    "    for i, sentence in enumerate(dev_model_sentences):\n",
    "        for j, word in enumerate(sentence):\n",
    "            f.write(f'{j+1} {word} {dev_tags[i][j]} {tag_list[pred_labels1[i][j]]}\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstmt = BLSTM2(len(glove_list), glove_embeddings).to(device)\n",
    "blstmt.load_state_dict(torch.load('blstm2.pt'))\n",
    "\n",
    "valid_data = LSTMTestDataset(embedded_dev, dev_masks)\n",
    "valid_loader = DataLoader(valid_data, batch_size=128)\n",
    "\n",
    "predicted_labels = get_test_preds(blstmt, valid_loader)\n",
    "with open('dev2.out', 'w') as f:\n",
    "    for i, sentence in enumerate(orig_dev_sentences):\n",
    "        for j, word in enumerate(sentence):\n",
    "            f.write(f'{j+1} {word} {tag_list[predicted_labels[i][j]]}\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "predicted_labels = get_test_preds(blstmt, test_loader)\n",
    "with open('test2.out','w') as f:\n",
    "    for i, sentence in enumerate(orig_test_sentences):\n",
    "        for j, word in enumerate(sentence):\n",
    "            f.write(f'{j+1} {word} {tag_list[predicted_labels[i][j]]}\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMCNNDataset(Dataset):\n",
    "#     def __init__(self, sentences, tags, masks, char_seq):\n",
    "#         self.sentences = pad_sequence([torch.tensor(sentence) for sentence in sentences], batch_first=True, padding_value=0)\n",
    "#         self.lengths = torch.tensor([len(sentence) for sentence in sentences])\n",
    "#         self.tags = pad_sequence([torch.tensor(tag) for tag in tags], batch_first=True, padding_value=-1)\n",
    "#         self.masks = pad_sequence([torch.tensor(mask) for mask in masks], batch_first=True, padding_value=2)\n",
    "#         self.char_seqs = char_seq\n",
    "#         # print(self.sentences.shape)\n",
    "#         # print(self.tags.shape)\n",
    "#         # print(self.masks.shape)\n",
    "#         # print(self.lengths.shape)\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.sentences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         sentence = self.sentences[idx]\n",
    "#         tag = self.tags[idx]\n",
    "#         length = self.lengths[idx]\n",
    "#         mask = self.masks[idx]\n",
    "#         char_seq = self.char_seqs[idx]\n",
    "#         return sentence, length, tag, mask, char_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_LSTMCNN(model, dataloader):\n",
    "#     predicted_labels = []\n",
    "#     valid_loss=0\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch, lens, labels, masks, char_seq in dataloader:\n",
    "#             packed_labels = pack_padded_sequence(labels, lens, batch_first=True, enforce_sorted=False)\n",
    "#             labels, len_labels = pad_packed_sequence(packed_labels, batch_first=True, padding_value=-1)\n",
    "#             batch, labels, masks = batch.to(device), labels.to(device), masks.to(device)\n",
    "#             y_pred = model(batch, lens, masks, char_seq)\n",
    "#             y_pred = torch.permute(y_pred, dims=(0,2,1))\n",
    "#             y_pred_class = torch.argmax(torch.log_softmax(y_pred, dim=1), dim=1)\n",
    "#             loss = loss_fn(y_pred, labels)\n",
    "#             valid_loss+= loss.item()\n",
    "#             predicted_labels.extend(y_pred_class.cpu().numpy().tolist())\n",
    "        \n",
    "#         valid_loss/=len(dataloader)\n",
    "#     print('Validation Loss : ', valid_loss)\n",
    "#     return predicted_labels, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trainLSTMCNN(model, optimizer, train_dataloader, valid_dataloader, scheduler=None, epochs=10):\n",
    "#     valid_loss_min = np.Inf\n",
    "#     for epoch in range(epochs):\n",
    "#         train_loss = 0\n",
    "\n",
    "#         model.train()\n",
    "#         for batch, lens, labels, masks, char_seq in train_dataloader:\n",
    "#             packed_labels = pack_padded_sequence(labels, lens, batch_first=True, enforce_sorted=False)\n",
    "#             labels,_ = pad_packed_sequence(packed_labels, batch_first=True, padding_value=-1)\n",
    "#             batch, labels, masks = batch.to(device), labels.to(device), masks.to(device)\n",
    "#             y_pred = model(batch, lens, masks, char_seq)\n",
    "#             y_pred = torch.permute(y_pred, (0,2,1))\n",
    "#             loss = loss_fn(y_pred, labels)\n",
    "#             train_loss += loss.item()\n",
    "#             optimizer.zero_grad()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "        \n",
    "#         train_loss/=len(train_dataloader)\n",
    "#         if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "#             scheduler.step(train_loss)\n",
    "#         elif scheduler is not None:\n",
    "#             scheduler.step()\n",
    "\n",
    "#         print('Epoch: {} \\tTraining Loss: {:.4f} \\t'.format(\n",
    "#         epoch+1, \n",
    "#         train_loss,\n",
    "#         ))\n",
    "#         _, valid_loss = evaluate_LSTMCNN(model, valid_dataloader)\n",
    "#         if valid_loss_min>valid_loss:\n",
    "#             print(f'Validation Loss Reduced ({valid_loss_min})--->({valid_loss}). Saving model')\n",
    "#             torch.save(model.state_dict(), 'blstmcnn.pt')\n",
    "#             valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedded_train = embed_input(train_model_sentences, glove_vocab, glove_list)\n",
    "# train_masks = boolean_mask(train_model_sentences)\n",
    "# encoded_train_tags = encode_tags(tagged_sentences)\n",
    "# train_encoded_chars = encode_char_sequence(train_sentences)\n",
    "\n",
    "# # print(len(embedded_train[0]), len(train_masks[0]), len(encoded_train_tags[0]))\n",
    "\n",
    "# train_data = LSTMCNNDataset(embedded_train, encoded_train_tags, train_masks, train_encoded_chars)\n",
    "# train_loader = DataLoader(train_data, batch_size=9, shuffle=True)\n",
    "\n",
    "# orig_dev_sentences, dev_model_sentences, dev_tags = process_dev('data/dev')\n",
    "# embedded_dev = embed_input(dev_model_sentences, glove_vocab, glove_list)\n",
    "# dev_masks = boolean_mask(dev_model_sentences)\n",
    "# encoded_dev = encode_tags(dev_tags)\n",
    "# dev_encoded_chars = encode_char_sequence(orig_dev_sentences)\n",
    "\n",
    "# valid_data = LSTMCNNDataset(embedded_dev, encoded_dev, dev_masks, dev_encoded_chars)\n",
    "# valid_loader = DataLoader(valid_data, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BLSTMCNN(nn.Module):\n",
    "#     def __init__(self, vocab_size, embeddings, cap_embed_dims=10, cnn_out=53, char_embed_dims=30):\n",
    "#         super(BLSTMCNN, self).__init__()\n",
    "#         self.char_embeds = nn.Embedding(len(char_list), char_embed_dims)\n",
    "#         nn.init.uniform_(self.char_embeds.weight, -np.sqrt(3/char_embed_dims),np.sqrt(3/char_embed_dims))\n",
    "#         self.cnn = nn.Sequential(\n",
    "#             nn.Conv1d(1, cnn_out, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.MaxPool1d(3)\n",
    "#         )\n",
    "#         self.embedding = nn.Embedding(vocab_size, 100, padding_idx=0)\n",
    "#         self.embedding.weight = nn.Parameter(torch.tensor(embeddings, dtype=torch.float))\n",
    "#         self.cap_embedding = nn.Embedding(3, cap_embed_dims, padding_idx=2)\n",
    "#         nn.init.uniform_(self.cap_embedding.weight, -np.sqrt(3/cap_embed_dims),np.sqrt(3/cap_embed_dims))\n",
    "#         self.lstm = nn.LSTM(100+cap_embed_dims+cnn_out, 256, batch_first=True, bidirectional=True)\n",
    "#         for name, param in self.lstm.named_parameters():\n",
    "#             if 'bias' in name:\n",
    "#                 nn.init.zeros_(param)\n",
    "#             elif 'weight' in name:\n",
    "#                 nn.init.uniform_(param, -np.sqrt(6/(len(param)+len(param[0]))), np.sqrt(6/(len(param)+len(param[0]))))\n",
    "#         self.dropout = nn.Dropout(0.33)\n",
    "#         self.linear = nn.Linear(512, 128)\n",
    "#         self.elu = nn.ELU()\n",
    "#         self.out = nn.Linear(128,9)\n",
    "\n",
    "#     def forward(self, x, lengths, masks, char_seq):\n",
    "#         # print('x shape ', x.shape)\n",
    "#         output = self.embedding(x)\n",
    "#         # print('after embedding ', output.shape)\n",
    "#         # print('masks shape ', masks.shape)\n",
    "#         # print(masks)\n",
    "#         cap_embeds = self.cap_embedding(masks)\n",
    "#         # print('cap embed shape ', cap_embeds.shape)\n",
    "#         char_embeds = self.char_embeds(char_seq)\n",
    "#         char_cnn = self.cnn(char_embeds)\n",
    "#         # print('char embed shape ', char_embed.shape)\n",
    "#         output = torch.cat((output, cap_embeds, char_cnn), 2)\n",
    "#         print('concatenated shape ', output.shape)\n",
    "#         output = pack_padded_sequence(output, lengths, batch_first=True, enforce_sorted=False)\n",
    "#         output, _ = self.lstm(output)\n",
    "#         output, _ = pad_packed_sequence(output, batch_first=True,padding_value=0)\n",
    "#         # print('after lstm ', output.shape)\n",
    "#         output = self.dropout(output)\n",
    "#         output = self.linear(output)\n",
    "#         # print('after linear ', output.shape)\n",
    "#         output = self.elu(output)\n",
    "#         output = self.out(output)\n",
    "#         # print('after classifier ', output.shape)\n",
    "#         return output\n",
    "    \n",
    "\n",
    "# lstmcnn = BLSTMCNN(len(glove_list), glove_embeddings).to(device)\n",
    "# optimizer3 = torch.optim.SGD(lstmcnn.parameters(), lr=0.5)\n",
    "# scheduler3 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer3,'min',0.5)\n",
    "# # scheduler2 = torch.optim.lr_scheduler.CyclicLR(optimizer2, 0.01, 0.1)\n",
    "# # scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer,5,0.5)\n",
    "        \n",
    "# trainLSTMCNN(lstmcnn, optimizer3, train_loader, valid_loader, scheduler3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstmcnn.load_state_dict(torch.load('blstmcnn.pt'))\n",
    "# pred_labels, _ = evaluate_LSTMCNN(lstmcnn, valid_loader)\n",
    "# with open('predictions3.txt','w') as f:\n",
    "#     for i, sentence in enumerate(dev_model_sentences):\n",
    "#         for j, word in enumerate(sentence):\n",
    "#             f.write(f'{j+1} {word} {dev_tags[i][j]} {tag_list[pred_labels[i][j]]}\\n')\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
