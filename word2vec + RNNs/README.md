### Tasks

1. Generated TF-IDF and word2vec features for Amazon Reviews. Compared the performance of both features to perform multi-class classification using Perceptron and SVM. Observed better accuracies using TF-IDF. This could be due to few reasons - TF-IDF is a statistical method that is intended to improve metrics such as precision and recall. Word2Vec is aimed at capturing semantic relationships. Google's pretrained word2vec was trained on a wide context that may not have seen all these emotions to weigh them well in vector space. 
2. Implement custom dataset classes and inherit dataloaders to enable training on PyTorch.
3. Implement MLP and analyse how to feed features to model. Tested with 2 ways - Average all vectors to generate a vector of same dimension for review; Concatenate the first 10 words to generate a vector of higher dimension for review. Observed that in the second case, the training loss decreases while validation loss increases. Experimented with dropout and lower learning rates but this phenomenon seems to occur regardless. The MLP performs significantly better than a single perceptron. Adding complexities to the model with non-linearity and layers has improved the performance from 48% to 66.1%. The disparity between the two cases is possibly because the first case tries to capture the entirety of a review while still maintaining lower dimensions whereas the latter concatenates the first 10 words, which may not contain all necessary information to classify, and also increases the dimensions of features.
4. Implement vanilla RNN, GRU and LSTM and compared performance of these models on the same classification task. GRU offers significant improvement over RNN due to the mechanism of gates that are capable of learning which inputs are important and need to be remembered. This additional gate mechanism solves the vanishing gradient problem of RNN but is causing the GRU to overfit on training data. LSTM is computationally slightly more expensive than GRU and offers solutions to the problem of overfitting by adding a forget gate. This results in better performance in both training and validation data.
