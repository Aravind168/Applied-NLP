{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install contractions\n",
    "# ! pip install scikit-learn\n",
    "# ! pip install pandas\n",
    "# ! pip install numpy\n",
    "# ! pip install nltk\n",
    "# ! pip install gensim\n",
    "# ! pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "# # Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/\n",
    "# #â†ªamazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karav\\Desktop\\Applied NLP\\HW3\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\karav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\karav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('amazon_reviews_us_Beauty_v1_00.tsv', sep=\"\\t\", encoding='utf-8', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['star_rating'] = data['star_rating'].astype('int',errors='ignore') \n",
    "\n",
    "# df = data[['review_body', 'star_rating']]\n",
    "# df = df.dropna(thresh=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form three classes of 20000 reviews randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_a = df.loc[df['star_rating'].isin([1,2])].sample(n=20000, random_state=4)\n",
    "# class_b = df.loc[df['star_rating'].isin([3])].sample(n=20000,random_state=7)\n",
    "# class_c = df.loc[df['star_rating'].isin([4,5])].sample(n=20000, random_state=21)\n",
    "\n",
    "# df_sampled = pd.concat([class_a, class_b, class_c])\n",
    "# df_sampled['star_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sampled['star_rating'] = df_sampled['star_rating'].replace([1,2],0)\n",
    "# df_sampled['star_rating'] = df_sampled['star_rating'].replace([3],1)\n",
    "# df_sampled['star_rating'] = df_sampled['star_rating'].replace([4,5],2)\n",
    "\n",
    "# df_sampled.to_csv(\"data.tsv\", sep='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, I have read the dataset from the tsv file. I converted the data type of star_rating field to\n",
    "int in order to enforce uniformity as there were some float and date values in them. After dropping\n",
    "null values, sampled 20000 reviews for each class label and merged them into a single dataframe and then stored it for easier read access."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to clean dataset by removing html, urls, punctuation and multiple spaces\n",
    "\n",
    "def remove_HTML(s):\n",
    "    return re.sub(r'<.*?>',' ',s)\n",
    "\n",
    "def remove_URL(s):\n",
    "    return re.sub(r'https?:\\/\\/.*\\/\\w*',' ',s)\n",
    "\n",
    "def remove_nonalphabets(s):\n",
    "    return re.sub(r'[^a-zA-Z]',' ',s)\n",
    "\n",
    "def remove_multispace(s):\n",
    "    return re.sub(r'\\s+',' ',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.tsv', sep='\\t', usecols=['review_body', 'star_rating'], dtype={'review_body' : str, 'star_rating': int})\n",
    "\n",
    "data['review_body'] = data['review_body'].apply(lambda x:remove_HTML(x))\n",
    "data['review_body'] = data['review_body'].apply(lambda x:remove_URL(x))\n",
    "data['review_body'] = data['review_body'].apply(lambda x:remove_nonalphabets(x))\n",
    "data['review_body'] = data['review_body'].apply(lambda x:remove_multispace(x))\n",
    "tfidfdata = data.copy(deep=True)\n",
    "data['review_body'] = data['review_body'].apply(lambda x: x.split())\n",
    "tfidfdata['review_body'] = tfidfdata['review_body'].apply(lambda x: x.lower().split())\n",
    "tfidfdata['review_body'] = tfidfdata['review_body'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "w2vdata = list(data['review_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(w2vdata, min_count=9, vector_size=300, window=13, workers=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Chinese', 0.8240145444869995), ('China', 0.6561532616615295), ('Indian', 0.643246054649353)]\n",
      "0.55674857\n",
      "0.37769592\n",
      "0.38830176\n",
      "poor\n",
      "excellent\n"
     ]
    }
   ],
   "source": [
    "#Testing examples on Google's word2vec\n",
    "\n",
    "test_vec = w2v['China']-w2v['India']+w2v['Indian']\n",
    "print(w2v.most_similar(positive=[test_vec],topn=3))\n",
    "print(w2v.similarity('excellent','outstanding'))\n",
    "print(w2v.similarity('excellent','poor'))\n",
    "print(w2v.similarity('beautiful','horrible'))\n",
    "print(w2v.doesnt_match(['excellent','outstanding','poor']))\n",
    "print(w2v.doesnt_match(['excellent','bad','poor']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('China', 0.9744254350662231), ('china', 0.7366096377372742), ('USA', 0.7354326248168945)]\n",
      "0.73620814\n",
      "0.5710197\n",
      "0.40271857\n",
      "poor\n",
      "bad\n"
     ]
    }
   ],
   "source": [
    "#Testing examples on trained word2vec\n",
    "\n",
    "test_vec = model.wv['China']-model.wv['India']+model.wv['Indian']\n",
    "print(model.wv.most_similar(positive=[test_vec],topn=3))\n",
    "print(model.wv.similarity('excellent','outstanding'))\n",
    "print(model.wv.similarity('excellent','poor'))\n",
    "print(model.wv.similarity('beautiful','horrible'))\n",
    "print(model.wv.doesnt_match(['excellent','outstanding','poor']))\n",
    "print(model.wv.doesnt_match(['excellent','bad','poor']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors generated by our dataset do not seem to capture the relationships between words accurately. Although it seems to place words that are relevant to our context in proximity in vector space, the relationships are not encoded properly. It is not capable of performing the vector algebra or picking odd one out as well as pretrained word2vec."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------TF-IDF Features--------------------\n",
      "Perceptron\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.69      0.65      4000\n",
      "           1       0.56      0.51      0.53      4000\n",
      "           2       0.71      0.70      0.70      4000\n",
      "\n",
      "    accuracy                           0.63     12000\n",
      "   macro avg       0.63      0.63      0.63     12000\n",
      "weighted avg       0.63      0.63      0.63     12000\n",
      "\n",
      "-------------------------------------------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71      4000\n",
      "           1       0.61      0.59      0.60      4000\n",
      "           2       0.76      0.79      0.77      4000\n",
      "\n",
      "    accuracy                           0.70     12000\n",
      "   macro avg       0.69      0.70      0.69     12000\n",
      "weighted avg       0.69      0.70      0.69     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Vectorizer = TfidfVectorizer()\n",
    "tfidf = Vectorizer.fit_transform(tfidfdata['review_body'])\n",
    "\n",
    "TF_train, TF_test, tf_train, tf_test  = train_test_split(tfidf, \n",
    "                            tfidfdata['star_rating'],\n",
    "                            stratify=tfidfdata['star_rating'],\n",
    "                            test_size=0.2, random_state=1)\n",
    "\n",
    "p = Perceptron(random_state=7)\n",
    "p.fit(TF_train, tf_train)\n",
    "\n",
    "print(\"--------------------TF-IDF Features--------------------\")\n",
    "print('Perceptron')\n",
    "print(classification_report(tf_test, p.predict(TF_test)))\n",
    "\n",
    "print('-------------------------------------------------------')\n",
    "print('SVM')\n",
    "s = LinearSVC(random_state=7, tol= 1e-5)\n",
    "s.fit(TF_train, tf_train)\n",
    "print(classification_report(tf_test, s.predict(TF_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tfidfdata, Vectorizer, tfidf, TF_train, tf_train, TF_test, tf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_w2v(data):\n",
    "    w2vfeats = []\n",
    "    for sentence in data:\n",
    "        word_vecs = [w2v[word] for word in sentence if word in w2v]\n",
    "        if len(word_vecs):\n",
    "            sent_vec = np.mean(word_vecs, axis=0)\n",
    "        else:\n",
    "            sent_vec = np.zeros(300)\n",
    "        w2vfeats.append(sent_vec)\n",
    "    return np.array(w2vfeats)\n",
    "\n",
    "w2vfeats_np = transform_w2v(w2vdata)\n",
    "w2vlabels_np = np.array(list(data['star_rating'].astype('int')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have averaged all the word vectors for each review. For reviews that have no words in the w2v vocabulary, the feature vector is a list of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(w2vfeats_np, \n",
    "                            w2vlabels_np,\n",
    "                            stratify=w2vlabels_np,\n",
    "                            test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Word2Vec Features--------------------\n",
      "Perceptron\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.19      0.30      4000\n",
      "           1       0.39      0.93      0.55      4000\n",
      "           2       0.87      0.33      0.48      4000\n",
      "\n",
      "    accuracy                           0.48     12000\n",
      "   macro avg       0.68      0.48      0.44     12000\n",
      "weighted avg       0.68      0.48      0.44     12000\n",
      "\n",
      "---------------------------------------------------------\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.68      0.67      4000\n",
      "           1       0.59      0.55      0.57      4000\n",
      "           2       0.71      0.72      0.72      4000\n",
      "\n",
      "    accuracy                           0.65     12000\n",
      "   macro avg       0.65      0.65      0.65     12000\n",
      "weighted avg       0.65      0.65      0.65     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------Word2Vec Features--------------------\")\n",
    "print('Perceptron')\n",
    "p = Perceptron(random_state=7)\n",
    "p.fit(X_train, y_train)\n",
    "print(classification_report(y_test, p.predict(X_test)))\n",
    "\n",
    "print(\"---------------------------------------------------------\")\n",
    "print('SVM')\n",
    "s = LinearSVC(random_state=7, tol= 1e-5)\n",
    "s.fit(X_train, y_train)\n",
    "print(classification_report(y_test, s.predict(X_test)))\n",
    "\n",
    "\n",
    "del p,s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF features - \n",
    "\n",
    "Perceptron : 63% accuracy | SVM : 70% accuracy\n",
    "\n",
    "Word2Vec features - \n",
    "\n",
    "Perceptron : 48% accuracy | SVM - 65% accuracy\n",
    "\n",
    "The models were able to classify better based on TF-IDF features than the word2vec features. This could be due to few reasons - TF-IDF is a statistical method that is intended to improve metrics such as precision and recall. Word2Vec is aimed at capturing semantic relationships. Google's pretrained word2vec was trained on a wide context that may not have seen all these emotions to weigh them well in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(device)\n",
    "# else:\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, transform=None):\n",
    "        self.reviews = torch.from_numpy(reviews).float()\n",
    "        self.labels = torch.from_numpy(labels).long()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            review = self.transform(review)\n",
    "        return review, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 64,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "}\n",
    "\n",
    "train_data = TrainDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, **params)\n",
    "\n",
    "valid_data = TrainDataset(X_test, y_test)\n",
    "valid_loader = DataLoader(valid_data, **params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a custom Dataset class inheriting from torch's Dataset and used a dataloader for batching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP1, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        # x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "mlp1 = MLP1()\n",
    "mlp1 = mlp1.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(mlp1.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.882419 \tValidation Loss: 0.805861 \tTraining Accuracy: 58.970833 \t Validation Accuracy: 64.666667\n",
      "Validation loss decreased (inf --> 0.805861).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.838510 \tValidation Loss: 0.840016 \tTraining Accuracy: 62.397917 \t Validation Accuracy: 62.116667\n",
      "Epoch: 3 \tTraining Loss: 0.823994 \tValidation Loss: 0.791876 \tTraining Accuracy: 62.995833 \t Validation Accuracy: 64.216667\n",
      "Validation loss decreased (0.805861 --> 0.791876).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.820845 \tValidation Loss: 0.786919 \tTraining Accuracy: 63.077083 \t Validation Accuracy: 64.991667\n",
      "Validation loss decreased (0.791876 --> 0.786919).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.811417 \tValidation Loss: 0.802333 \tTraining Accuracy: 63.672917 \t Validation Accuracy: 65.258333\n",
      "Epoch: 6 \tTraining Loss: 0.803838 \tValidation Loss: 0.788787 \tTraining Accuracy: 64.104167 \t Validation Accuracy: 64.850000\n",
      "Epoch: 7 \tTraining Loss: 0.800308 \tValidation Loss: 0.787960 \tTraining Accuracy: 64.539583 \t Validation Accuracy: 63.950000\n",
      "Epoch: 8 \tTraining Loss: 0.799988 \tValidation Loss: 0.773323 \tTraining Accuracy: 64.083333 \t Validation Accuracy: 65.500000\n",
      "Validation loss decreased (0.786919 --> 0.773323).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.795391 \tValidation Loss: 0.783199 \tTraining Accuracy: 64.558333 \t Validation Accuracy: 65.475000\n",
      "Epoch: 10 \tTraining Loss: 0.793381 \tValidation Loss: 0.767247 \tTraining Accuracy: 64.854167 \t Validation Accuracy: 65.966667\n",
      "Validation loss decreased (0.773323 --> 0.767247).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.792889 \tValidation Loss: 0.769620 \tTraining Accuracy: 64.683333 \t Validation Accuracy: 65.750000\n",
      "Epoch: 12 \tTraining Loss: 0.789712 \tValidation Loss: 0.780824 \tTraining Accuracy: 64.731250 \t Validation Accuracy: 64.975000\n",
      "Epoch: 13 \tTraining Loss: 0.788710 \tValidation Loss: 0.782966 \tTraining Accuracy: 64.991667 \t Validation Accuracy: 64.800000\n",
      "Epoch: 14 \tTraining Loss: 0.787117 \tValidation Loss: 0.771606 \tTraining Accuracy: 64.793750 \t Validation Accuracy: 65.833333\n",
      "Epoch: 15 \tTraining Loss: 0.787902 \tValidation Loss: 0.774148 \tTraining Accuracy: 65.154167 \t Validation Accuracy: 65.391667\n",
      "Epoch: 16 \tTraining Loss: 0.781290 \tValidation Loss: 0.775664 \tTraining Accuracy: 65.266667 \t Validation Accuracy: 65.608333\n",
      "Epoch: 17 \tTraining Loss: 0.779198 \tValidation Loss: 0.769034 \tTraining Accuracy: 65.466667 \t Validation Accuracy: 66.341667\n",
      "Epoch: 18 \tTraining Loss: 0.780767 \tValidation Loss: 0.762907 \tTraining Accuracy: 65.554167 \t Validation Accuracy: 66.291667\n",
      "Validation loss decreased (0.767247 --> 0.762907).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.780964 \tValidation Loss: 0.763635 \tTraining Accuracy: 65.177083 \t Validation Accuracy: 65.883333\n",
      "Epoch: 20 \tTraining Loss: 0.780560 \tValidation Loss: 0.796080 \tTraining Accuracy: 65.337500 \t Validation Accuracy: 63.791667\n",
      "Epoch: 21 \tTraining Loss: 0.776748 \tValidation Loss: 0.764099 \tTraining Accuracy: 65.718750 \t Validation Accuracy: 66.158333\n",
      "Epoch: 22 \tTraining Loss: 0.780137 \tValidation Loss: 0.767298 \tTraining Accuracy: 65.433333 \t Validation Accuracy: 66.300000\n",
      "Epoch: 23 \tTraining Loss: 0.771557 \tValidation Loss: 0.765212 \tTraining Accuracy: 65.685417 \t Validation Accuracy: 65.925000\n",
      "Epoch: 24 \tTraining Loss: 0.774043 \tValidation Loss: 0.765254 \tTraining Accuracy: 65.566667 \t Validation Accuracy: 66.341667\n",
      "Epoch: 25 \tTraining Loss: 0.772692 \tValidation Loss: 0.799089 \tTraining Accuracy: 65.539583 \t Validation Accuracy: 65.008333\n",
      "Epoch: 26 \tTraining Loss: 0.773041 \tValidation Loss: 0.766370 \tTraining Accuracy: 65.804167 \t Validation Accuracy: 66.200000\n",
      "Epoch: 27 \tTraining Loss: 0.770929 \tValidation Loss: 0.765811 \tTraining Accuracy: 65.929167 \t Validation Accuracy: 66.216667\n",
      "Epoch: 28 \tTraining Loss: 0.773404 \tValidation Loss: 0.773933 \tTraining Accuracy: 65.641667 \t Validation Accuracy: 65.400000\n",
      "Epoch: 29 \tTraining Loss: 0.773131 \tValidation Loss: 0.784065 \tTraining Accuracy: 65.754167 \t Validation Accuracy: 65.333333\n",
      "Epoch: 30 \tTraining Loss: 0.773960 \tValidation Loss: 0.760797 \tTraining Accuracy: 65.477083 \t Validation Accuracy: 66.108333\n",
      "Validation loss decreased (0.762907 --> 0.760797).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 30\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    mlp1.train()\n",
    "    train_correct=0\n",
    "    for batch, labels in train_loader:\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = mlp1(batch)\n",
    "        loss = loss_fn(output, labels)\n",
    "        predicted = torch.argmax(output, dim=1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+= loss.item()\n",
    "        train_correct+= (predicted==labels).sum().item()\n",
    "\n",
    "    mlp1.eval()\n",
    "    valid_correct=0\n",
    "    with torch.no_grad():\n",
    "        for batch, labels in valid_loader:\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            output = mlp1(batch)\n",
    "            loss = loss_fn(output, labels)\n",
    "            predicted = torch.argmax(output, dim=1)\n",
    "            valid_loss+= loss.item()\n",
    "            valid_correct+=(predicted==labels).sum().item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    valid_loss = valid_loss/len(valid_loader)\n",
    "    train_acc = 100 * train_correct / len(train_data)\n",
    "    valid_acc = 100 * valid_correct / len(valid_data)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTraining Accuracy: {:.6f} \\t Validation Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        train_acc,\n",
    "        valid_acc\n",
    "        ))\n",
    "\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(mlp1.state_dict(), 'mlp1.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "del mlp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('mlp1.pt')) to load best valid_loss model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_w2v_10words(data):\n",
    "    w2vfeats = []\n",
    "    for sentence in data:\n",
    "        word_vecs = [w2v[word] for word in sentence if word in w2v]\n",
    "        sent_vec = np.concatenate([word_vecs[i] if i < len(word_vecs) else np.zeros(300) for i in range(10)], axis=0)\n",
    "        w2vfeats.append(sent_vec)\n",
    "    return np.array(w2vfeats)\n",
    "\n",
    "w2vfeats_np = transform_w2v_10words(w2vdata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenated only first 10 words to generate a vector for reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(w2vfeats_np, \n",
    "                            w2vlabels_np,\n",
    "                            stratify=w2vlabels_np,\n",
    "                            test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 64,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "}\n",
    "\n",
    "train_data = TrainDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, **params)\n",
    "\n",
    "valid_data = TrainDataset(X_test, y_test)\n",
    "valid_loader = DataLoader(valid_data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP2, self).__init__()\n",
    "        self.fc1 = nn.Linear(3000, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "mlp2 = MLP2()\n",
    "mlp2 = mlp2.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(mlp2.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.976678 \tValidation Loss: 0.924451 \tTraining Accuracy: 50.704167 \t Validation Accuracy: 54.841667\n",
      "Validation loss decreased (inf --> 0.924451).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.917402 \tValidation Loss: 0.902421 \tTraining Accuracy: 56.052083 \t Validation Accuracy: 56.175000\n",
      "Validation loss decreased (0.924451 --> 0.902421).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.884060 \tValidation Loss: 0.900157 \tTraining Accuracy: 57.893750 \t Validation Accuracy: 56.175000\n",
      "Validation loss decreased (0.902421 --> 0.900157).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.850590 \tValidation Loss: 0.909461 \tTraining Accuracy: 59.993750 \t Validation Accuracy: 56.275000\n",
      "Epoch: 5 \tTraining Loss: 0.820458 \tValidation Loss: 0.920390 \tTraining Accuracy: 61.864583 \t Validation Accuracy: 56.175000\n",
      "Epoch: 6 \tTraining Loss: 0.790396 \tValidation Loss: 0.934739 \tTraining Accuracy: 63.789583 \t Validation Accuracy: 54.891667\n",
      "Epoch: 7 \tTraining Loss: 0.762101 \tValidation Loss: 0.934167 \tTraining Accuracy: 65.118750 \t Validation Accuracy: 55.950000\n",
      "Epoch: 8 \tTraining Loss: 0.743555 \tValidation Loss: 0.948798 \tTraining Accuracy: 66.289583 \t Validation Accuracy: 56.308333\n",
      "Epoch: 9 \tTraining Loss: 0.715384 \tValidation Loss: 0.999014 \tTraining Accuracy: 67.735417 \t Validation Accuracy: 55.400000\n",
      "Epoch: 10 \tTraining Loss: 0.692863 \tValidation Loss: 0.995963 \tTraining Accuracy: 69.102083 \t Validation Accuracy: 54.825000\n",
      "Epoch: 11 \tTraining Loss: 0.673906 \tValidation Loss: 0.999969 \tTraining Accuracy: 69.691667 \t Validation Accuracy: 55.408333\n",
      "Epoch: 12 \tTraining Loss: 0.662720 \tValidation Loss: 1.002574 \tTraining Accuracy: 70.443750 \t Validation Accuracy: 55.166667\n",
      "Epoch: 13 \tTraining Loss: 0.643923 \tValidation Loss: 1.020750 \tTraining Accuracy: 71.410417 \t Validation Accuracy: 54.875000\n",
      "Epoch: 14 \tTraining Loss: 0.632360 \tValidation Loss: 1.061070 \tTraining Accuracy: 72.102083 \t Validation Accuracy: 55.141667\n",
      "Epoch: 15 \tTraining Loss: 0.622716 \tValidation Loss: 1.056160 \tTraining Accuracy: 72.402083 \t Validation Accuracy: 55.425000\n",
      "Epoch: 16 \tTraining Loss: 0.607511 \tValidation Loss: 1.085207 \tTraining Accuracy: 73.291667 \t Validation Accuracy: 55.341667\n",
      "Epoch: 17 \tTraining Loss: 0.598487 \tValidation Loss: 1.114869 \tTraining Accuracy: 73.583333 \t Validation Accuracy: 55.158333\n",
      "Epoch: 18 \tTraining Loss: 0.591536 \tValidation Loss: 1.111149 \tTraining Accuracy: 73.964583 \t Validation Accuracy: 54.691667\n",
      "Epoch: 19 \tTraining Loss: 0.581423 \tValidation Loss: 1.144386 \tTraining Accuracy: 74.447917 \t Validation Accuracy: 54.616667\n",
      "Epoch: 20 \tTraining Loss: 0.579383 \tValidation Loss: 1.146685 \tTraining Accuracy: 74.779167 \t Validation Accuracy: 54.716667\n",
      "Epoch: 21 \tTraining Loss: 0.569262 \tValidation Loss: 1.178676 \tTraining Accuracy: 75.316667 \t Validation Accuracy: 55.358333\n",
      "Epoch: 22 \tTraining Loss: 0.561153 \tValidation Loss: 1.165771 \tTraining Accuracy: 75.602083 \t Validation Accuracy: 55.191667\n",
      "Epoch: 23 \tTraining Loss: 0.553000 \tValidation Loss: 1.181841 \tTraining Accuracy: 75.793750 \t Validation Accuracy: 53.525000\n",
      "Epoch: 24 \tTraining Loss: 0.550625 \tValidation Loss: 1.216485 \tTraining Accuracy: 76.018750 \t Validation Accuracy: 54.425000\n",
      "Epoch: 25 \tTraining Loss: 0.541611 \tValidation Loss: 1.220801 \tTraining Accuracy: 76.566667 \t Validation Accuracy: 54.675000\n",
      "Epoch: 26 \tTraining Loss: 0.539132 \tValidation Loss: 1.136490 \tTraining Accuracy: 76.789583 \t Validation Accuracy: 54.591667\n",
      "Epoch: 27 \tTraining Loss: 0.530489 \tValidation Loss: 1.258120 \tTraining Accuracy: 77.212500 \t Validation Accuracy: 54.616667\n",
      "Epoch: 28 \tTraining Loss: 0.533744 \tValidation Loss: 1.198413 \tTraining Accuracy: 76.943750 \t Validation Accuracy: 54.591667\n",
      "Epoch: 29 \tTraining Loss: 0.526562 \tValidation Loss: 1.253449 \tTraining Accuracy: 77.431250 \t Validation Accuracy: 54.316667\n",
      "Epoch: 30 \tTraining Loss: 0.524036 \tValidation Loss: 1.316890 \tTraining Accuracy: 77.437500 \t Validation Accuracy: 55.033333\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 30\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    mlp2.train()\n",
    "    train_correct=0\n",
    "    for batch, labels in train_loader:\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = mlp2(batch)\n",
    "        loss = loss_fn(output, labels)\n",
    "        predicted = torch.argmax(output, dim=1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+= loss.item()\n",
    "        train_correct+= (predicted==labels).sum().item()\n",
    "\n",
    "    mlp2.eval()\n",
    "    valid_correct=0\n",
    "    with torch.no_grad():\n",
    "        for batch, labels in valid_loader:\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            output = mlp2(batch)\n",
    "            loss = loss_fn(output, labels)\n",
    "            predicted = torch.argmax(output, dim=1)\n",
    "            valid_loss+= loss.item()\n",
    "            valid_correct+=(predicted==labels).sum().item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    valid_loss = valid_loss/len(valid_loader)\n",
    "    train_acc = 100 * train_correct / len(train_data)\n",
    "    valid_acc = 100 * valid_correct / len(valid_data)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTraining Accuracy: {:.6f} \\t Validation Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        train_acc,\n",
    "        valid_acc\n",
    "        ))\n",
    "\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(mlp2.state_dict(), 'mlp2.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "del mlp2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed that in the case of 4(b), the training loss decreases while validation loss increases. Experimented with dropout and lower learning rates but this phenomenon seems to occur regardless. The model in 4(a) performs significantly better than a single perceptron. Adding complexities to the model with non-linearity and layers has improved the performance from 48% to 66.1%. The disparity between 4(a) and 4(b) is possibly because 4(a) tries to capture the entirety of a review while still maintaining lower dimensions whereas 4(b) concatenates the first 10 words, which may not contain all necessary information to classify, and also increases the dimensions of features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_rnn(data):\n",
    "    w2vfeats = []\n",
    "    for sentence in data:    \n",
    "        word_vecs = [w2v[word] for word in sentence if word in w2v]\n",
    "        sent_vec = [word_vecs[i] if i < len(word_vecs) else np.zeros(300) for i in range(20)]\n",
    "        w2vfeats.append(sent_vec)\n",
    "    return np.array(w2vfeats)\n",
    "\n",
    "w2vfeats_np = transform_rnn(w2vdata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created word2vec features for review by appending 20 word vectors individually without performing any mathematical operations. If there are less than 20 words, the feature is padded with zeros; If there are more than 20, review is truncated at 20 words. This is the input required for RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(w2vfeats_np, \n",
    "                            w2vlabels_np,\n",
    "                            stratify=w2vlabels_np,\n",
    "                            test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 64,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "}\n",
    "\n",
    "train_data = TrainDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, **params)\n",
    "\n",
    "valid_data = TrainDataset(X_test, y_test)\n",
    "valid_loader = DataLoader(valid_data, **params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, 1, batch_first=True, nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = F.log_softmax(self.fc(out[:,-1,:]), dim=1) #Last output alone to calculate loss\n",
    "        return out\n",
    "\n",
    "rnn = RNN(300, 20, 3)\n",
    "rnn = rnn.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(rnn.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.973483 \tValidation Loss: 0.924852 \tTraining Accuracy: 50.408333 \t Validation Accuracy: 55.008333\n",
      "Validation loss decreased (inf --> 0.924852).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.884785 \tValidation Loss: 0.866593 \tTraining Accuracy: 57.091667 \t Validation Accuracy: 57.658333\n",
      "Validation loss decreased (0.924852 --> 0.866593).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.859548 \tValidation Loss: 0.863384 \tTraining Accuracy: 59.220833 \t Validation Accuracy: 58.991667\n",
      "Validation loss decreased (0.866593 --> 0.863384).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.841908 \tValidation Loss: 0.880996 \tTraining Accuracy: 60.531250 \t Validation Accuracy: 58.741667\n",
      "Epoch: 5 \tTraining Loss: 0.828599 \tValidation Loss: 0.850196 \tTraining Accuracy: 61.337500 \t Validation Accuracy: 60.008333\n",
      "Validation loss decreased (0.863384 --> 0.850196).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.822405 \tValidation Loss: 0.841068 \tTraining Accuracy: 61.489583 \t Validation Accuracy: 60.483333\n",
      "Validation loss decreased (0.850196 --> 0.841068).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.811936 \tValidation Loss: 0.874489 \tTraining Accuracy: 62.456250 \t Validation Accuracy: 57.950000\n",
      "Epoch: 8 \tTraining Loss: 0.805039 \tValidation Loss: 0.833688 \tTraining Accuracy: 63.006250 \t Validation Accuracy: 60.850000\n",
      "Validation loss decreased (0.841068 --> 0.833688).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.802850 \tValidation Loss: 0.843161 \tTraining Accuracy: 62.750000 \t Validation Accuracy: 60.425000\n",
      "Epoch: 10 \tTraining Loss: 0.797369 \tValidation Loss: 0.834855 \tTraining Accuracy: 63.375000 \t Validation Accuracy: 60.158333\n",
      "Epoch: 11 \tTraining Loss: 0.795914 \tValidation Loss: 0.861628 \tTraining Accuracy: 63.385417 \t Validation Accuracy: 60.258333\n",
      "Epoch: 12 \tTraining Loss: 0.799824 \tValidation Loss: 0.824695 \tTraining Accuracy: 63.541667 \t Validation Accuracy: 61.400000\n",
      "Validation loss decreased (0.833688 --> 0.824695).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.792791 \tValidation Loss: 0.839745 \tTraining Accuracy: 63.675000 \t Validation Accuracy: 60.766667\n",
      "Epoch: 14 \tTraining Loss: 0.787382 \tValidation Loss: 0.826666 \tTraining Accuracy: 64.014583 \t Validation Accuracy: 61.275000\n",
      "Epoch: 15 \tTraining Loss: 0.786393 \tValidation Loss: 0.870654 \tTraining Accuracy: 64.162500 \t Validation Accuracy: 58.650000\n",
      "Epoch: 16 \tTraining Loss: 0.798419 \tValidation Loss: 0.863961 \tTraining Accuracy: 63.493750 \t Validation Accuracy: 60.275000\n",
      "Epoch: 17 \tTraining Loss: 0.783020 \tValidation Loss: 0.878881 \tTraining Accuracy: 64.277083 \t Validation Accuracy: 58.891667\n",
      "Epoch: 18 \tTraining Loss: 0.781670 \tValidation Loss: 0.831605 \tTraining Accuracy: 64.418750 \t Validation Accuracy: 61.716667\n",
      "Epoch: 19 \tTraining Loss: 0.780028 \tValidation Loss: 0.915492 \tTraining Accuracy: 64.408333 \t Validation Accuracy: 55.991667\n",
      "Epoch: 20 \tTraining Loss: 0.776635 \tValidation Loss: 0.826320 \tTraining Accuracy: 64.731250 \t Validation Accuracy: 61.233333\n",
      "Epoch: 21 \tTraining Loss: 0.796501 \tValidation Loss: 0.842651 \tTraining Accuracy: 63.597917 \t Validation Accuracy: 61.108333\n",
      "Epoch: 22 \tTraining Loss: 0.780859 \tValidation Loss: 0.835370 \tTraining Accuracy: 64.397917 \t Validation Accuracy: 61.625000\n",
      "Epoch: 23 \tTraining Loss: 0.778767 \tValidation Loss: 0.820345 \tTraining Accuracy: 64.720833 \t Validation Accuracy: 62.083333\n",
      "Validation loss decreased (0.824695 --> 0.820345).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.779166 \tValidation Loss: 1.127440 \tTraining Accuracy: 64.633333 \t Validation Accuracy: 36.166667\n",
      "Epoch: 25 \tTraining Loss: 0.835730 \tValidation Loss: 0.844096 \tTraining Accuracy: 60.952083 \t Validation Accuracy: 60.116667\n",
      "Epoch: 26 \tTraining Loss: 0.786725 \tValidation Loss: 0.831346 \tTraining Accuracy: 64.439583 \t Validation Accuracy: 61.166667\n",
      "Epoch: 27 \tTraining Loss: 0.777375 \tValidation Loss: 0.825478 \tTraining Accuracy: 64.979167 \t Validation Accuracy: 61.791667\n",
      "Epoch: 28 \tTraining Loss: 0.840067 \tValidation Loss: 0.850965 \tTraining Accuracy: 61.150000 \t Validation Accuracy: 60.283333\n",
      "Epoch: 29 \tTraining Loss: 0.806351 \tValidation Loss: 0.876543 \tTraining Accuracy: 63.060417 \t Validation Accuracy: 59.841667\n",
      "Epoch: 30 \tTraining Loss: 0.787632 \tValidation Loss: 0.832393 \tTraining Accuracy: 64.231250 \t Validation Accuracy: 61.600000\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 30\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    rnn.train()\n",
    "    \n",
    "    train_correct=0\n",
    "    for batch, labels in train_loader:\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = rnn(batch)\n",
    "        # print(output.shape, '|', labels.shape)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+= loss.item()\n",
    "        train_correct+= (torch.argmax(output, dim=1)==labels).sum().item()\n",
    "\n",
    "    rnn.eval()\n",
    "    valid_correct=0\n",
    "    with torch.no_grad():\n",
    "        for batch, labels in valid_loader:\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            output = rnn(batch)\n",
    "            loss=loss_fn(output, labels)\n",
    "            valid_loss+= loss.item()\n",
    "            valid_correct+= (torch.argmax(output, dim=1)==labels).sum().item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    valid_loss = valid_loss/len(valid_loader)\n",
    "    train_acc = 100 * train_correct / len(train_data)\n",
    "    valid_acc = 100 * valid_correct / len(valid_data)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTraining Accuracy: {:.6f} \\t Validation Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        train_acc,\n",
    "        valid_acc\n",
    "        ))\n",
    "\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(rnn.state_dict(), 'rnn.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFNN - 66.1% | RNN - 61.4% (Accuracy of models with best validation loss)\n",
    "\n",
    "RNN accuracy is slightly lower as compared to FFNN. The training loss is decreasing while the validation loss is increasing. Experimented by adding dropout and reducing learning rate but RNN still seems to try to overfit on the data. The discrepancy in accuracy values could be possibly due to RNN only considering 20 words whereas the FFNN averages the entire review."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = F.log_softmax(self.fc(out[:,-1,:]), dim=1) #Taking the last output alone to calculate loss\n",
    "        return out\n",
    "\n",
    "gru = GRU(300, 20, 2, 3)\n",
    "gru = gru.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(gru.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.852240 \tValidation Loss: 0.782885 \tTraining Accuracy: 59.668750 \t Validation Accuracy: 63.900000\n",
      "Validation loss decreased (inf --> 0.782885).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.758170 \tValidation Loss: 0.759034 \tTraining Accuracy: 65.779167 \t Validation Accuracy: 65.200000\n",
      "Validation loss decreased (0.782885 --> 0.759034).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.726154 \tValidation Loss: 0.749555 \tTraining Accuracy: 67.587500 \t Validation Accuracy: 66.350000\n",
      "Validation loss decreased (0.759034 --> 0.749555).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.703309 \tValidation Loss: 0.748379 \tTraining Accuracy: 68.800000 \t Validation Accuracy: 66.275000\n",
      "Validation loss decreased (0.749555 --> 0.748379).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.682284 \tValidation Loss: 0.742501 \tTraining Accuracy: 69.950000 \t Validation Accuracy: 66.800000\n",
      "Validation loss decreased (0.748379 --> 0.742501).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.665869 \tValidation Loss: 0.742956 \tTraining Accuracy: 70.700000 \t Validation Accuracy: 66.650000\n",
      "Epoch: 7 \tTraining Loss: 0.652505 \tValidation Loss: 0.772581 \tTraining Accuracy: 71.654167 \t Validation Accuracy: 65.325000\n",
      "Epoch: 8 \tTraining Loss: 0.639380 \tValidation Loss: 0.760187 \tTraining Accuracy: 72.245833 \t Validation Accuracy: 66.441667\n",
      "Epoch: 9 \tTraining Loss: 0.625784 \tValidation Loss: 0.774371 \tTraining Accuracy: 72.816667 \t Validation Accuracy: 65.750000\n",
      "Epoch: 10 \tTraining Loss: 0.615435 \tValidation Loss: 0.779434 \tTraining Accuracy: 73.287500 \t Validation Accuracy: 66.025000\n",
      "Epoch: 11 \tTraining Loss: 0.608425 \tValidation Loss: 0.765682 \tTraining Accuracy: 73.835417 \t Validation Accuracy: 65.916667\n",
      "Epoch: 12 \tTraining Loss: 0.597958 \tValidation Loss: 0.787514 \tTraining Accuracy: 74.368750 \t Validation Accuracy: 65.916667\n",
      "Epoch: 13 \tTraining Loss: 0.589329 \tValidation Loss: 0.792497 \tTraining Accuracy: 74.758333 \t Validation Accuracy: 65.850000\n",
      "Epoch: 14 \tTraining Loss: 0.585013 \tValidation Loss: 0.811597 \tTraining Accuracy: 75.120833 \t Validation Accuracy: 65.950000\n",
      "Epoch: 15 \tTraining Loss: 0.576118 \tValidation Loss: 0.823616 \tTraining Accuracy: 75.637500 \t Validation Accuracy: 64.666667\n",
      "Epoch: 16 \tTraining Loss: 0.571046 \tValidation Loss: 0.810080 \tTraining Accuracy: 75.852083 \t Validation Accuracy: 65.333333\n",
      "Epoch: 17 \tTraining Loss: 0.566572 \tValidation Loss: 0.841204 \tTraining Accuracy: 75.881250 \t Validation Accuracy: 65.541667\n",
      "Epoch: 18 \tTraining Loss: 0.558577 \tValidation Loss: 0.852746 \tTraining Accuracy: 76.381250 \t Validation Accuracy: 65.258333\n",
      "Epoch: 19 \tTraining Loss: 0.556633 \tValidation Loss: 0.848621 \tTraining Accuracy: 76.427083 \t Validation Accuracy: 65.125000\n",
      "Epoch: 20 \tTraining Loss: 0.552553 \tValidation Loss: 0.839151 \tTraining Accuracy: 76.752083 \t Validation Accuracy: 65.091667\n",
      "Epoch: 21 \tTraining Loss: 0.544731 \tValidation Loss: 0.857952 \tTraining Accuracy: 77.156250 \t Validation Accuracy: 64.750000\n",
      "Epoch: 22 \tTraining Loss: 0.543537 \tValidation Loss: 0.849694 \tTraining Accuracy: 77.129167 \t Validation Accuracy: 63.841667\n",
      "Epoch: 23 \tTraining Loss: 0.540733 \tValidation Loss: 0.864469 \tTraining Accuracy: 77.168750 \t Validation Accuracy: 64.833333\n",
      "Epoch: 24 \tTraining Loss: 0.534506 \tValidation Loss: 0.867184 \tTraining Accuracy: 77.456250 \t Validation Accuracy: 64.716667\n",
      "Epoch: 25 \tTraining Loss: 0.533053 \tValidation Loss: 0.864193 \tTraining Accuracy: 77.564583 \t Validation Accuracy: 64.391667\n",
      "Epoch: 26 \tTraining Loss: 0.526711 \tValidation Loss: 0.874959 \tTraining Accuracy: 77.825000 \t Validation Accuracy: 64.550000\n",
      "Epoch: 27 \tTraining Loss: 0.525434 \tValidation Loss: 0.901615 \tTraining Accuracy: 78.062500 \t Validation Accuracy: 64.108333\n",
      "Epoch: 28 \tTraining Loss: 0.523132 \tValidation Loss: 0.892494 \tTraining Accuracy: 78.016667 \t Validation Accuracy: 63.983333\n",
      "Epoch: 29 \tTraining Loss: 0.521680 \tValidation Loss: 0.907037 \tTraining Accuracy: 78.102083 \t Validation Accuracy: 63.641667\n",
      "Epoch: 30 \tTraining Loss: 0.517928 \tValidation Loss: 0.903834 \tTraining Accuracy: 78.370833 \t Validation Accuracy: 64.258333\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 30\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    gru.train()\n",
    "    \n",
    "    train_correct=0\n",
    "    for batch, labels in train_loader:\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = gru(batch)\n",
    "        # print(output.shape, '|', labels.shape)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+= loss.item()\n",
    "        train_correct+= (torch.argmax(output, dim=1)==labels).sum().item()\n",
    "\n",
    "    gru.eval()\n",
    "    valid_correct=0\n",
    "    with torch.no_grad():\n",
    "        for batch, labels in valid_loader:\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            output = gru(batch)\n",
    "            loss=loss_fn(output, labels)\n",
    "            valid_loss+= loss.item()\n",
    "            valid_correct+= (torch.argmax(output, dim=1)==labels).sum().item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    valid_loss = valid_loss/len(valid_loader)\n",
    "    train_acc = 100 * train_correct / len(train_data)\n",
    "    valid_acc = 100 * valid_correct / len(valid_data)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTraining Accuracy: {:.6f} \\t Validation Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        train_acc,\n",
    "        valid_acc\n",
    "        ))\n",
    "\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(gru.state_dict(), 'gru.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU - 66.8% (Accuracy after 5 epochs with best validation loss)\n",
    "\n",
    "Validation loss is increasing with more epochs while training loss is decreasing, showing signs of overfitting. At end of 30 epochs, validation accuracy is 64.25% which is still better than a simple RNN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        cell_state = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        out, (hidden, cell_state) = self.lstm(x, (hidden, cell_state))\n",
    "        # print(out.shape)\n",
    "        out = F.log_softmax(self.fc(out[:,-1,:]), dim=1) #Taking the last output alone to calculate loss\n",
    "        return out\n",
    "\n",
    "lstm = LSTM(300, 20, 2, 3)\n",
    "lstm = lstm.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(lstm.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.943884 \tValidation Loss: 0.885581 \tTraining Accuracy: 52.981250 \t Validation Accuracy: 57.966667\n",
      "Validation loss decreased (inf --> 0.885581).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.834866 \tValidation Loss: 0.825576 \tTraining Accuracy: 61.293750 \t Validation Accuracy: 61.766667\n",
      "Validation loss decreased (0.885581 --> 0.825576).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.797658 \tValidation Loss: 0.809182 \tTraining Accuracy: 63.581250 \t Validation Accuracy: 62.716667\n",
      "Validation loss decreased (0.825576 --> 0.809182).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.777975 \tValidation Loss: 0.789377 \tTraining Accuracy: 64.693750 \t Validation Accuracy: 63.858333\n",
      "Validation loss decreased (0.809182 --> 0.789377).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.760450 \tValidation Loss: 0.779643 \tTraining Accuracy: 65.758333 \t Validation Accuracy: 64.375000\n",
      "Validation loss decreased (0.789377 --> 0.779643).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.745741 \tValidation Loss: 0.768820 \tTraining Accuracy: 66.468750 \t Validation Accuracy: 64.991667\n",
      "Validation loss decreased (0.779643 --> 0.768820).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.730878 \tValidation Loss: 0.771671 \tTraining Accuracy: 67.454167 \t Validation Accuracy: 64.791667\n",
      "Epoch: 8 \tTraining Loss: 0.721521 \tValidation Loss: 0.767454 \tTraining Accuracy: 67.639583 \t Validation Accuracy: 65.216667\n",
      "Validation loss decreased (0.768820 --> 0.767454).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.711574 \tValidation Loss: 0.776195 \tTraining Accuracy: 68.450000 \t Validation Accuracy: 64.541667\n",
      "Epoch: 10 \tTraining Loss: 0.701459 \tValidation Loss: 0.762840 \tTraining Accuracy: 68.931250 \t Validation Accuracy: 65.566667\n",
      "Validation loss decreased (0.767454 --> 0.762840).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.691796 \tValidation Loss: 0.765836 \tTraining Accuracy: 69.410417 \t Validation Accuracy: 65.475000\n",
      "Epoch: 12 \tTraining Loss: 0.685782 \tValidation Loss: 0.751288 \tTraining Accuracy: 69.700000 \t Validation Accuracy: 66.100000\n",
      "Validation loss decreased (0.762840 --> 0.751288).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.677693 \tValidation Loss: 0.761019 \tTraining Accuracy: 70.185417 \t Validation Accuracy: 65.950000\n",
      "Epoch: 14 \tTraining Loss: 0.671068 \tValidation Loss: 0.745408 \tTraining Accuracy: 70.512500 \t Validation Accuracy: 66.375000\n",
      "Validation loss decreased (0.751288 --> 0.745408).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.663661 \tValidation Loss: 0.759947 \tTraining Accuracy: 70.770833 \t Validation Accuracy: 65.541667\n",
      "Epoch: 16 \tTraining Loss: 0.658640 \tValidation Loss: 0.755860 \tTraining Accuracy: 70.827083 \t Validation Accuracy: 65.866667\n",
      "Epoch: 17 \tTraining Loss: 0.650487 \tValidation Loss: 0.764561 \tTraining Accuracy: 71.500000 \t Validation Accuracy: 65.891667\n",
      "Epoch: 18 \tTraining Loss: 0.645009 \tValidation Loss: 0.761992 \tTraining Accuracy: 71.833333 \t Validation Accuracy: 65.991667\n",
      "Epoch: 19 \tTraining Loss: 0.639872 \tValidation Loss: 0.770475 \tTraining Accuracy: 72.125000 \t Validation Accuracy: 66.391667\n",
      "Epoch: 20 \tTraining Loss: 0.634310 \tValidation Loss: 0.763986 \tTraining Accuracy: 72.445833 \t Validation Accuracy: 66.008333\n",
      "Epoch: 21 \tTraining Loss: 0.627621 \tValidation Loss: 0.780688 \tTraining Accuracy: 72.775000 \t Validation Accuracy: 65.991667\n",
      "Epoch: 22 \tTraining Loss: 0.624477 \tValidation Loss: 0.776929 \tTraining Accuracy: 72.868750 \t Validation Accuracy: 66.208333\n",
      "Epoch: 23 \tTraining Loss: 0.620246 \tValidation Loss: 0.820825 \tTraining Accuracy: 73.133333 \t Validation Accuracy: 64.758333\n",
      "Epoch: 24 \tTraining Loss: 0.617020 \tValidation Loss: 0.778197 \tTraining Accuracy: 73.147917 \t Validation Accuracy: 65.983333\n",
      "Epoch: 25 \tTraining Loss: 0.609854 \tValidation Loss: 0.776264 \tTraining Accuracy: 73.545833 \t Validation Accuracy: 65.333333\n",
      "Epoch: 26 \tTraining Loss: 0.605385 \tValidation Loss: 0.793560 \tTraining Accuracy: 73.808333 \t Validation Accuracy: 66.241667\n",
      "Epoch: 27 \tTraining Loss: 0.600010 \tValidation Loss: 0.784294 \tTraining Accuracy: 74.104167 \t Validation Accuracy: 66.375000\n",
      "Epoch: 28 \tTraining Loss: 0.596917 \tValidation Loss: 0.782613 \tTraining Accuracy: 74.316667 \t Validation Accuracy: 66.016667\n",
      "Epoch: 29 \tTraining Loss: 0.591855 \tValidation Loss: 0.787732 \tTraining Accuracy: 74.533333 \t Validation Accuracy: 66.041667\n",
      "Epoch: 30 \tTraining Loss: 0.589975 \tValidation Loss: 0.788648 \tTraining Accuracy: 74.785417 \t Validation Accuracy: 66.116667\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 30\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    lstm.train()\n",
    "    \n",
    "    train_correct=0\n",
    "    for batch, labels in train_loader:\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = lstm(batch)\n",
    "        # print(output.shape, '|', labels.shape)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+= loss.item()\n",
    "        train_correct+= (torch.argmax(output, dim=1)==labels).sum().item()\n",
    "\n",
    "    lstm.eval()\n",
    "    valid_correct=0\n",
    "    with torch.no_grad():\n",
    "        for batch, labels in valid_loader:\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            output = lstm(batch)\n",
    "            loss=loss_fn(output, labels)\n",
    "            valid_loss+= loss.item()\n",
    "            valid_correct+= (torch.argmax(output, dim=1)==labels).sum().item()\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    valid_loss = valid_loss/len(valid_loader)\n",
    "    train_acc = 100 * train_correct / len(train_data)\n",
    "    valid_acc = 100 * valid_correct / len(valid_data)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTraining Accuracy: {:.6f} \\t Validation Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        train_acc,\n",
    "        valid_acc\n",
    "        ))\n",
    "\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(lstm.state_dict(), 'lstm.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN - Best model : 61.4% accuracy | GRU - Best model : 66.8% accuracy | LSTM - Best model : 66.37% accuracy\n",
    "\n",
    "GRU offers significant improvement over RNN due to the mechanism of gates that are capable of learning which inputs are important and need to be remembered. This additional gate mechanism solves the vanishing gradient problem of RNN but is causing the GRU to overfit on training data.\n",
    "\n",
    "LSTM is computationally slightly more expensive than GRU and offers solutions to the problem of overfitting by adding a forget gate. This results in better performance in both training and validation data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "2. https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook\n",
    "3. https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "4. https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "5. https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05b5e7d9a0f56b0b0679eceb96a14c59b6af1872a8a0a79dab778f5faa5e6201"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
